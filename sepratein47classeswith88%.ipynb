{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c722ff53d6ce9d3efd795d0b27622d6f2731cca8"},"cell_type":"markdown","source":"Firstly, we need to load our data, notice that there is no column names in csv files and thus header shold be set to `None`."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"train = pd.read_csv('../input/emnist-balanced-train.csv', header=None)\ntest = pd.read_csv('../input/emnist-balanced-test.csv', header=None)\ntrain.head()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1de1e3161fbb5508d50795ae5ec111fe9dece93"},"cell_type":"markdown","source":"Now split labels and images from original dataframe."},{"metadata":{"trusted":true,"_uuid":"873961cc8c68e670d8da4e1bc8f481b4285fd695"},"cell_type":"code","source":"train_data = train.iloc[:, 1:]\ntrain_labels = train.iloc[:, 0]\ntest_data = test.iloc[:, 1:]\ntest_labels = test.iloc[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ce317f24e54de329fc8c70b29d5680327902eb3"},"cell_type":"markdown","source":"One hot encoding with `get_dummies()` and you can compare it with the original labels."},{"metadata":{"trusted":true,"_uuid":"f579b8562191e84691329a20f9641ac67efbbb14"},"cell_type":"code","source":"train_labels = pd.get_dummies(train_labels)\ntest_labels = pd.get_dummies(test_labels)\ntrain_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69cfc5f3f284179a4267d12099f7f75427191101"},"cell_type":"markdown","source":"Turn our Dataframes into numpy array and delete `train` and `test` to save up memory."},{"metadata":{"trusted":true,"_uuid":"99bd8b190cb539cb791c9679e71a63aab2740dd1"},"cell_type":"code","source":"train_data = train_data.values\ntrain_labels = train_labels.values\ntest_data = test_data.values\ntest_labels = test_labels.values\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac3a3dcac965dec9dabb7e1ac7455e67055f8aca"},"cell_type":"markdown","source":"For some reason, sadly, the EMNIST dataset was rotated and flipped and we need fix that."},{"metadata":{"trusted":true,"_uuid":"378e0eba6eed6a4ffc1f684dcd205f0d7eecc7af"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.imshow(train_data[45].reshape([28, 28]), cmap='Greys_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"202b20574a27ac0b98de387605a18af6e945080b"},"cell_type":"code","source":"def rotate(image):\n    image = image.reshape([28, 28])\n    image = np.fliplr(image)\n    image = np.rot90(image)\n    return image.reshape([28 * 28])\ntrain_data = np.apply_along_axis(rotate, 1, train_data)/255\ntest_data = np.apply_along_axis(rotate, 1, test_data)/255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"808004dba9959f4571844c2292b0990b94074fe0"},"cell_type":"code","source":"plt.imshow(train_data[45].reshape([28, 28]), cmap='Greys_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adf2ae2680fd52ba850af9554d66eb621f0e9bb1"},"cell_type":"markdown","source":"Now let's import tensorflow to start building our network, with slim we can keep our code neat."},{"metadata":{"trusted":true,"_uuid":"dc4e22b26f9c79d66c53d27c7a01ec8c1ceebe02"},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74dbac5263d3f949a01c7ed1ca732baaf78bf802"},"cell_type":"code","source":"tf.reset_default_graph()\nxs = tf.placeholder(tf.float32, [None, 784], name='input')\nys = tf.placeholder(tf.float32, [None, 47], name='exp_output')\ndropout = tf.placeholder(tf.float32, name='dropout')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2c45977878920e266dfb7e5c1b141480d230f6f"},"cell_type":"code","source":" # define the computation graph\nx_image = tf.reshape(xs, [-1, 28, 28, 1])\nlayer = tf.layers.conv2d(x_image, 64, [5,5], padding='same', activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nlayer = tf.layers.max_pooling2d(layer, pool_size=(2,2), strides=2) # [-1, 14, 14, 64]\nlayer = tf.layers.batch_normalization(layer)\nlayer = tf.layers.conv2d(layer, 128, [2,2], padding='same', activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nlayer = tf.layers.max_pooling2d(layer, pool_size=(2,2), strides=2) # [-1, 7, 7, 128]\nx_flat = tf.reshape(layer, [-1, 7*7*128])\nflatten = tf.layers.dense(x_flat, 1024, activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nflatten = tf.nn.dropout(flatten, keep_prob=1-dropout)\nflatten = tf.layers.dense(flatten, 512, activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nflatten = tf.layers.batch_normalization(flatten)\nflatten = tf.layers.dense(flatten, 128, activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nflatten = tf.layers.dense(flatten, 47)\npred = tf.nn.softmax(flatten, name='output')\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbd4bce186ab31cd63e970d3c9c7a22247ac9158"},"cell_type":"markdown","source":"Now define our loss function, in this case, it is `softmax_cross_entropy_with_logits` but the official document shows it is deprecated as labels will be affected in backprop. But we use placeholders to feed data, and this is will not be a problem for us. In order to get rid of the warn info and keep our notebook neat, I use the `_v2` version. For more information on these two functions, you can refer to [StackExchange](https://stats.stackexchange.com/questions/327348/how-is-softmax-cross-entropy-with-logits-different-from-softmax-cross-entropy-wi)"},{"metadata":{"trusted":true,"_uuid":"b5905a309da68737c34fa739f0b64a3208b857d5"},"cell_type":"code","source":"cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n    labels=ys,\n    logits=flatten))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0c9f29e9491ea634f11f13efa296ee576165b35"},"cell_type":"code","source":"train = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\ncorrect = tf.equal(tf.argmax(flatten, 1), tf.argmax(ys, 1))\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aff502a3d10b81ff0065a4e906d076157525d35c"},"cell_type":"code","source":"init = tf.global_variables_initializer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6292e1428355587fa910c0ced89fc4e0b7599ce"},"cell_type":"code","source":"NUM = 112800\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(20):\n        for i in range(int(NUM / 100)):\n            x_batches, y_batches = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n            sess.run(train, feed_dict={xs: x_batches, ys: y_batches, dropout: 0.5})\n            \n            if i % 1000 == 0:\n                acc, entropy = sess.run([accuracy, cross_entropy], feed_dict={xs: test_data,\n                                                    ys: test_labels,\n                                                    dropout: 0})\n                print('Train Entropy : ', sess.run(cross_entropy, feed_dict={xs: x_batches, ys: y_batches, dropout: 0.5}))\n                print('Test Accr & Entropy : ', acc, entropy)\n                # save_and_generate_proto(sess)\n    acc = sess.run(accuracy, feed_dict={xs: test_data,\n                                                ys: test_labels,\n                                                dropout: 0})\n    print(acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f744eff99b8e0c46c9f81bbc7601f032ce008098"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8fd281b37e9ea882924bd2b72bdce8dd5925ef8"},"cell_type":"markdown","source":"The final accuracy of our network is about 88.27%.  We are using the same balanced dataset and achieved these results. We have normalized the input pixels by dividing them with 255, hence a boost in accuracy was achieved from 85 to 88%. We have also used batch norms to tackle with the problem of covarient shifts. From our training results we conclude that the model has high varience, it is evident from the fact that training loss got down to 0.15 and testing loss was 3 times more than training loss. With this we conclude either our model was overfitting or that train data is not enough. Since we have used dropouts, overfitting is not an issue, The issue is the dataset, We can gather more training data and that should possibly solve the problem of high varience."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}